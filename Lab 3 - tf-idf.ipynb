{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "720e4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math #important library , calculate log\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bd950e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scientists have discovered a new species of marine life in the deep ocean.',\n",
       " \"NASA's Mars rover is searching for signs of ancient life on the Red Planet.\",\n",
       " 'The stock market experienced a significant drop in trading today.',\n",
       " 'Astronomers have identified a distant galaxy with unusual star formations.',\n",
       " 'The government announced new measures to combat climate change.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample collection of documents\n",
    "documents = [\n",
    "    \"Scientists have discovered a new species of marine life in the deep ocean.\",\n",
    "    \"NASA's Mars rover is searching for signs of ancient life on the Red Planet.\",\n",
    "    \"The stock market experienced a significant drop in trading today.\",\n",
    "    \"Astronomers have identified a distant galaxy with unusual star formations.\",\n",
    "    \"The government announced new measures to combat climate change.\"\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5ea50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for lemmatization (a simple example, not comprehensive)\n",
    "lemmatization_dict = {\n",
    "    \"species\": \"specie\",\n",
    "    \"species\": \"species\",\n",
    "    \"oceans\": \"ocean\",\n",
    "    \"ocean's\": \"ocean\",\n",
    "    \"rover\": \"rover\",\n",
    "    \"discovered\":\"discover\",\n",
    "    \"experienced\":\"experience\",\n",
    "    \"rovers\": \"rover\",\n",
    "    \"trading\": \"trade\",\n",
    "    \"identified\": \"identify\",\n",
    "    \"identifies\": \"identify\",\n",
    "    \"formations\": \"formation\",\n",
    "    \"governments\": \"government\",\n",
    "    \"measures\": \"measure\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16f1f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms = [lemmatization_dict.get(term, term) for term in terms]\n",
    "# terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed247b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['scientists',\n",
       "  'have',\n",
       "  'discover',\n",
       "  'a',\n",
       "  'new',\n",
       "  'species',\n",
       "  'of',\n",
       "  'marine',\n",
       "  'life',\n",
       "  'in',\n",
       "  'the',\n",
       "  'deep',\n",
       "  'ocean'],\n",
       " [\"nasa's\",\n",
       "  'mars',\n",
       "  'rover',\n",
       "  'is',\n",
       "  'searching',\n",
       "  'for',\n",
       "  'signs',\n",
       "  'of',\n",
       "  'ancient',\n",
       "  'life',\n",
       "  'on',\n",
       "  'the',\n",
       "  'red',\n",
       "  'planet'],\n",
       " ['the',\n",
       "  'stock',\n",
       "  'market',\n",
       "  'experience',\n",
       "  'a',\n",
       "  'significant',\n",
       "  'drop',\n",
       "  'in',\n",
       "  'trade',\n",
       "  'today'],\n",
       " ['astronomers',\n",
       "  'have',\n",
       "  'identify',\n",
       "  'a',\n",
       "  'distant',\n",
       "  'galaxy',\n",
       "  'with',\n",
       "  'unusual',\n",
       "  'star',\n",
       "  'formation'],\n",
       " ['the',\n",
       "  'government',\n",
       "  'announced',\n",
       "  'new',\n",
       "  'measure',\n",
       "  'to',\n",
       "  'combat',\n",
       "  'climate',\n",
       "  'change']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize documents into words (terms), remove punctuation, and lemmatize\n",
    "def preprocess_text(document):\n",
    "    terms = document.lower().split()\n",
    "    terms = [term.strip(string.punctuation) for term in terms]\n",
    "    terms = [lemmatization_dict.get(term, term) for term in terms]\n",
    "    return terms\n",
    "list_docs = [preprocess_text(document) for document in documents]\n",
    "list_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d60347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ancient',\n",
       " 'announced',\n",
       " 'astronomers',\n",
       " 'change',\n",
       " 'climate',\n",
       " 'combat',\n",
       " 'deep',\n",
       " 'discover',\n",
       " 'distant',\n",
       " 'drop',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'formation',\n",
       " 'galaxy',\n",
       " 'government',\n",
       " 'have',\n",
       " 'identify',\n",
       " 'in',\n",
       " 'is',\n",
       " 'life',\n",
       " 'marine',\n",
       " 'market',\n",
       " 'mars',\n",
       " 'measure',\n",
       " \"nasa's\",\n",
       " 'new',\n",
       " 'ocean',\n",
       " 'of',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'red',\n",
       " 'rover',\n",
       " 'scientists',\n",
       " 'searching',\n",
       " 'significant',\n",
       " 'signs',\n",
       " 'species',\n",
       " 'star',\n",
       " 'stock',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'trade',\n",
       " 'unusual',\n",
       " 'with'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a set of unique terms (vocabulary)\n",
    "vocabulary = set(y for x in list_docs for y in x)\n",
    "\n",
    "\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34cee75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ancient',\n",
       " 'announced',\n",
       " 'astronomers',\n",
       " 'change',\n",
       " 'climate',\n",
       " 'combat',\n",
       " 'deep',\n",
       " 'discover',\n",
       " 'distant',\n",
       " 'drop',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'formation',\n",
       " 'galaxy',\n",
       " 'government',\n",
       " 'have',\n",
       " 'identify',\n",
       " 'in',\n",
       " 'is',\n",
       " 'life',\n",
       " 'marine',\n",
       " 'market',\n",
       " 'mars',\n",
       " 'measure',\n",
       " \"nasa's\",\n",
       " 'new',\n",
       " 'ocean',\n",
       " 'of',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'red',\n",
       " 'rover',\n",
       " 'scientists',\n",
       " 'searching',\n",
       " 'significant',\n",
       " 'signs',\n",
       " 'species',\n",
       " 'star',\n",
       " 'stock',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'trade',\n",
       " 'unusual',\n",
       " 'with'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a set of unique terms (vocabulary)\n",
    "vocabulary = set()\n",
    "# you code here ...\n",
    "for x in documents:\n",
    "    y=preprocess_text(x)\n",
    "    vocabulary.update(y)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da0c442a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'significant': [0, 0, 0, 0, 0],\n",
       " 'climate': [0, 0, 0, 0, 0],\n",
       " 'astronomers': [0, 0, 0, 0, 0],\n",
       " \"nasa's\": [0, 0, 0, 0, 0],\n",
       " 'identify': [0, 0, 0, 0, 0],\n",
       " 'measure': [0, 0, 0, 0, 0],\n",
       " 'drop': [0, 0, 0, 0, 0],\n",
       " 'searching': [0, 0, 0, 0, 0],\n",
       " 'change': [0, 0, 0, 0, 0],\n",
       " 'on': [0, 0, 0, 0, 0],\n",
       " 'experience': [0, 0, 0, 0, 0],\n",
       " 'star': [0, 0, 0, 0, 0],\n",
       " 'announced': [0, 0, 0, 0, 0],\n",
       " 'market': [0, 0, 0, 0, 0],\n",
       " 'marine': [0, 0, 0, 0, 0],\n",
       " 'new': [0, 0, 0, 0, 0],\n",
       " 'ancient': [0, 0, 0, 0, 0],\n",
       " 'in': [0, 0, 0, 0, 0],\n",
       " 'discover': [0, 0, 0, 0, 0],\n",
       " 'trade': [0, 0, 0, 0, 0],\n",
       " 'signs': [0, 0, 0, 0, 0],\n",
       " 'the': [0, 0, 0, 0, 0],\n",
       " 'government': [0, 0, 0, 0, 0],\n",
       " 'life': [0, 0, 0, 0, 0],\n",
       " 'unusual': [0, 0, 0, 0, 0],\n",
       " 'rover': [0, 0, 0, 0, 0],\n",
       " 'today': [0, 0, 0, 0, 0],\n",
       " 'for': [0, 0, 0, 0, 0],\n",
       " 'stock': [0, 0, 0, 0, 0],\n",
       " 'scientists': [0, 0, 0, 0, 0],\n",
       " 'is': [0, 0, 0, 0, 0],\n",
       " 'distant': [0, 0, 0, 0, 0],\n",
       " 'combat': [0, 0, 0, 0, 0],\n",
       " 'galaxy': [0, 0, 0, 0, 0],\n",
       " 'species': [0, 0, 0, 0, 0],\n",
       " 'to': [0, 0, 0, 0, 0],\n",
       " 'ocean': [0, 0, 0, 0, 0],\n",
       " 'deep': [0, 0, 0, 0, 0],\n",
       " 'red': [0, 0, 0, 0, 0],\n",
       " 'planet': [0, 0, 0, 0, 0],\n",
       " 'have': [0, 0, 0, 0, 0],\n",
       " 'of': [0, 0, 0, 0, 0],\n",
       " 'mars': [0, 0, 0, 0, 0],\n",
       " 'formation': [0, 0, 0, 0, 0],\n",
       " 'a': [0, 0, 0, 0, 0],\n",
       " 'with': [0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store the term frequency (TF) for each term in each document\n",
    "tf_values = {term: [0] * len(documents) for term in vocabulary}\n",
    "tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fc32da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Term Frequency (TF)\n",
    "for i, document in enumerate(documents):\n",
    "    terms = preprocess_text(document)\n",
    "    for term in terms:\n",
    "        tf_values[term][i] +=1\n",
    "      \n",
    "            \n",
    "#     for term in vocabulary:\n",
    "#         if term in document:\n",
    "#             tf_values = {term: [] * len(documents) for term in vocabulary}\n",
    "#         else:\n",
    "#             tf_values.append(0) \n",
    "                \n",
    "        \n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860933f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11d91fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "significant : 0.9162907318741551\n",
      "climate : 0.9162907318741551\n",
      "astronomers : 0.9162907318741551\n",
      "nasa's : 0.9162907318741551\n",
      "identify : 0.9162907318741551\n",
      "measure : 0.9162907318741551\n",
      "drop : 0.9162907318741551\n",
      "searching : 0.9162907318741551\n",
      "change : 0.9162907318741551\n",
      "on : 0.9162907318741551\n",
      "experience : 0.9162907318741551\n",
      "star : 0.9162907318741551\n",
      "announced : 0.9162907318741551\n",
      "market : 0.9162907318741551\n",
      "marine : 0.9162907318741551\n",
      "new : 0.5108256237659907\n",
      "ancient : 0.9162907318741551\n",
      "in : 0.5108256237659907\n",
      "discover : 0.9162907318741551\n",
      "trade : 0.9162907318741551\n",
      "signs : 0.9162907318741551\n",
      "the : 0.0\n",
      "government : 0.9162907318741551\n",
      "life : 0.5108256237659907\n",
      "unusual : 0.9162907318741551\n",
      "rover : 0.9162907318741551\n",
      "today : 0.9162907318741551\n",
      "for : 0.9162907318741551\n",
      "stock : 0.9162907318741551\n",
      "scientists : 0.9162907318741551\n",
      "is : 0.9162907318741551\n",
      "distant : 0.9162907318741551\n",
      "combat : 0.9162907318741551\n",
      "galaxy : 0.9162907318741551\n",
      "species : 0.9162907318741551\n",
      "to : 0.9162907318741551\n",
      "ocean : 0.9162907318741551\n",
      "deep : 0.9162907318741551\n",
      "red : 0.9162907318741551\n",
      "planet : 0.9162907318741551\n",
      "have : 0.5108256237659907\n",
      "of : 0.5108256237659907\n",
      "mars : 0.9162907318741551\n",
      "formation : 0.9162907318741551\n",
      "a : 0.22314355131420976\n",
      "with : 0.9162907318741551\n"
     ]
    }
   ],
   "source": [
    "# Calculate Inverse Document Frequency (IDF)\n",
    "idf_values = {}\n",
    "total_documents = len(documents)\n",
    "for term in vocabulary:\n",
    "    document_occurences = sum([1 for document in documents if term in preprocess_text(document)])\n",
    "    idf_values[term] = math.log(total_documents / (document_occurences+1))\n",
    "    print(term,\":\",idf_values[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be824663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22314355131420976,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22314355131420976,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.22314355131420976,\n",
       "  0.9162907318741551],\n",
       " [0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.5108256237659907,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.9162907318741551,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate TF-IDF values\n",
    "tfidf_values = []\n",
    "for i, document in enumerate(documents):\n",
    "    terms = preprocess_text(document)\n",
    "    tfidf_document = []\n",
    "    for term in vocabulary:\n",
    "        tf = tf_values[term][i]\n",
    "        idf = idf_values[term]\n",
    "        tfidf = tf * idf\n",
    "        tfidf_document.append(tfidf)\n",
    "    tfidf_values.append(tfidf_document)\n",
    "tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f53d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "   significant   climate  astronomers    nasa's  identify   measure      drop  \\\n",
      "0     0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1     0.000000  0.000000     0.000000  0.916291  0.000000  0.000000  0.000000   \n",
      "2     0.916291  0.000000     0.000000  0.000000  0.000000  0.000000  0.916291   \n",
      "3     0.000000  0.000000     0.916291  0.000000  0.916291  0.000000  0.000000   \n",
      "4     0.000000  0.916291     0.000000  0.000000  0.000000  0.916291  0.000000   \n",
      "\n",
      "   searching    change        on  ...     ocean      deep       red    planet  \\\n",
      "0   0.000000  0.000000  0.000000  ...  0.916291  0.916291  0.000000  0.000000   \n",
      "1   0.916291  0.000000  0.916291  ...  0.000000  0.000000  0.916291  0.916291   \n",
      "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.916291  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       have        of      mars  formation         a      with  \n",
      "0  0.510826  0.510826  0.000000   0.000000  0.223144  0.000000  \n",
      "1  0.000000  0.510826  0.916291   0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000   0.000000  0.223144  0.000000  \n",
      "3  0.510826  0.000000  0.000000   0.916291  0.223144  0.916291  \n",
      "4  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert TF-IDF values to a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_values, columns=list(vocabulary))\n",
    "\n",
    "# Display TF-IDF results\n",
    "print(\"TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fcbe8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF results to a CSV file (optional)\n",
    "# df_tfidf.to_csv(\"tfidf_custom_preprocessed_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36077c4e",
   "metadata": {},
   "source": [
    "# Using Libraries for Lemmatization and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0495e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if your machine doesn't have these libraries, you need to install them\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the punkt\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69087c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/macbookpro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize NLTK's lemmatizer and download stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "# Initialize NLTK's lemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Tokenize documents into words (terms), remove punctuation, lemmatize, and remove stopwords\n",
    "def preprocess_text(document):\n",
    "    terms = nltk.word_tokenize(document)\n",
    "    terms = [term.strip(string.punctuation) for term in terms]\n",
    "    terms = [ps.stem(term) for term in terms]\n",
    "    terms = [term.lower() for term in terms if term not in stopwords.words('english')]\n",
    "    return ' '.join(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c050db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2385865948.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[38], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    preprocessed_documents = # your code here ...\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text in the documents\n",
    "preprocessed_documents = preprocessed_documents\n",
    "\n",
    "# Create a TfidfVectorizer instance\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ded965f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit and transform the preprocessed documents to compute TF-IDF values CADT@0zJanZ!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(preprocessed_documents)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the TF-IDF matrix to a DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_tfidf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit and transform the preprocessed documents to compute TF-IDF values CADT@0zJanZ!\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2e4c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "   significant   climate  astronomers    nasa's  identify   measure      drop  \\\n",
      "0     0.000000  0.000000     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1     0.000000  0.000000     0.000000  0.916291  0.000000  0.000000  0.000000   \n",
      "2     0.916291  0.000000     0.000000  0.000000  0.000000  0.000000  0.916291   \n",
      "3     0.000000  0.000000     0.916291  0.000000  0.916291  0.000000  0.000000   \n",
      "4     0.000000  0.916291     0.000000  0.000000  0.000000  0.916291  0.000000   \n",
      "\n",
      "   searching    change        on  ...     ocean      deep       red    planet  \\\n",
      "0   0.000000  0.000000  0.000000  ...  0.916291  0.916291  0.000000  0.000000   \n",
      "1   0.916291  0.000000  0.916291  ...  0.000000  0.000000  0.916291  0.916291   \n",
      "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4   0.000000  0.916291  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "       have        of      mars  formation         a      with  \n",
      "0  0.510826  0.510826  0.000000   0.000000  0.223144  0.000000  \n",
      "1  0.000000  0.510826  0.916291   0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000   0.000000  0.223144  0.000000  \n",
      "3  0.510826  0.000000  0.000000   0.916291  0.223144  0.916291  \n",
      "4  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display TF-IDF results\n",
    "print(\"TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd49cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76162981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35932909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ea796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
